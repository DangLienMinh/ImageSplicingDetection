\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
\chaptermark{Introduction}

Recently advanced image processing tools and computer graphics techniques make it straightforward to edit or modify digital images. In a forensics scenario, this raises the challenge of discriminating original images from malicious forgeries. Particular region from an image is pasted into other image with purpose to create image splicing. 

Image splicing is a common type of image tampering (manipulation) operation. The image integrity verification as well as identifying the areas of tampering on images without need to any expert support or manual process or prior knowledge original image contents is now days becoming the challenging research problem.

Investigating image's lighting is one of the most common approaches for splicing detection. This approach is particularly robust since it's really hard to preserve the consistency of the lighting environment while creating an image composite (i.e. a splicing forgery). 

In this scenario, there are mainly two main approaches:
\begin{enumerate}
\item based on the object-light geometric arrangement
\item based on illuminant colors
\end{enumerate}

We focused our attention on the illuminant-based approach, which assumes that a scene is lit by the same light source. More light sources are admitted but far enough such as to produce a constant brightness across the image. In this condition, pristine images will show a coherent illuminant representation; on the other hand, inconsistencies among illuminant maps will be exploited for splicing detection. 

\emph{Illuminant Maps} locally describes the lighting in a small region of the image. In the computer vision literature exists many different approaches for determining the illuminant of an image has been proposed. In particular, such techniques are divided into two main groups: statistical-based and physics-based approaches.

Regarding the first group, we start investigating on the \emph{Grey-World algorithm} \cite{Buchsbaum19801}, which is based on the Grey-World assumption, i.e. the average reflectance in a scene is achromatic. In \cite{finlayson2004shades}, this algorithm proved to be special instances of the Minkowski-norm. Van de Weijer et al. \cite{van2007edge} than proposed an extension of the Gray-World assumption, called \emph{Gray-Edge hypothesi}s \cite{van2007edge}, which assumes that the average of the reflectance differences in a scene is achromatic. The reflectance differences can be determined by taking derivatives of the image. Therefore, the authors present a framework with which many different algorithms can be constructed.
We focus our attention on the last case, called generalized \emph{Grey-World algorithm (GGE)}. The resulting illuminant maps presents also global illuminant features because of the gray-world and grey-edge assumptions.

For the latter group, was investigate the method proposed by Riess et al. \cite{riess2010scene}, which extends the \emph{Inverse Intense Chromaticity} (\emph{IIC}) space approach proposed by Tan et al. \cite{tan2004color} and tries to model the illuminants considering the dichromatic reflection model \cite{tominaga1989standard}. In this case, the illuminant map is evaluated dividing  images into blocks, named superpixels, of approximately the same object color, then the illuminant color is evaluated for each block solving the lighting models locally. 

Carvalho et al. \cite{carvalho2016illuminant} then presents a method that relies on a combination of the two approaches for the detection of manipulations on images containing human faces. In addition to maps, a large set of shape and texture descriptors are used together. Note that, from a theoretical viewpoint, it is advantageous to consider only image regions that consist of approximately the same underlying material: for this reason, in \cite{carvalho2016illuminant} the authors focused their analysis on human faces.

In \cite{carvalho2016illuminant} it is also shown that the difference between the two maps, GGE and IIC, increased when fake images are processed. This insight leads to the idea that it is possible to localize tampered image regions simply by considering IM differences with some metric, avoiding the computation of multiple descriptors.